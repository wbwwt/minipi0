import torch
import json
import os
from pathlib import Path
from transformers import AutoTokenizer

# 1. å¯¼å…¥æ‚¨çš„è‡ªå®šä¹‰æ¨¡å—
try:
    from model.modeling_pi0 import PI0Policy
    print("âœ… æˆåŠŸå¯¼å…¥ model.modeling_pi0")
except ImportError as e:
    print(f"âŒ å¯¼å…¥ model å¤±è´¥: {e}")
    exit(1)

try:
    from utils.normalization import Normalizer
    print("âœ… æˆåŠŸå¯¼å…¥ utils.normalization")
except ImportError as e:
    print(f"âŒ å¯¼å…¥ utils å¤±è´¥: {e}")
    exit(1)

# ================= é…ç½®è·¯å¾„ =================
# è¯·æ ¹æ®æ‚¨å®é™…çš„æƒé‡è·¯å¾„ä¿®æ”¹è¿™é‡Œ
MODEL_PATH = "/root/Users/wangbo/pi0"
# é€šå¸¸ stats æ–‡ä»¶åœ¨æ¨¡å‹ç›®å½•ä¸‹ï¼Œå« dataset_stats.json æˆ– stats.json
STATS_PATH = f"{MODEL_PATH}/dataset_stats.json" 

TOKENIZER_PATH = "/root/Users/wangbo/lerobot/tokenizers/paligemma"
# ===========================================

def main():
    print(f"\nğŸš€ å¼€å§‹å…¨ç³»ç»ŸéªŒè¯...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âš™ï¸  è¿è¡Œè®¾å¤‡: {device}")

    # ---------------------------------------------------------
    # æ­¥éª¤ 1: éªŒè¯ Normalizer (utils/normalization.py)
    # ---------------------------------------------------------
    print("\n[Step 1] éªŒè¯å½’ä¸€åŒ–æ¨¡å—...")
    normalizer = None
    if os.path.exists(STATS_PATH):
        try:
            normalizer = Normalizer(STATS_PATH, device=device)
            print(f"âœ… Normalizer åŠ è½½æˆåŠŸ! (è¯»å–è‡ª {STATS_PATH})")
            # ç®€å•æ£€æŸ¥å†…å®¹
            if "action" in normalizer.stats:
                print(f"   - åŒ…å« Action ç»Ÿè®¡: Mean shape {normalizer.stats['action']['mean'].shape}")
            else:
                print("âš ï¸  è­¦å‘Š: ç»Ÿè®¡æ–‡ä»¶ä¸­æ²¡æœ‰ 'action' å­—æ®µ")
        except Exception as e:
            print(f"âŒ Normalizer åˆå§‹åŒ–å‡ºé”™: {e}")
            return
    else:
        print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°ç»Ÿè®¡æ–‡ä»¶ {STATS_PATH}ï¼Œå°†è·³è¿‡æ•°å€¼è¿˜åŸæµ‹è¯•ã€‚")

    # ---------------------------------------------------------
    # æ­¥éª¤ 2: éªŒè¯æ¨¡å‹åŠ è½½ (model/modeling_pi0.py)
    # ---------------------------------------------------------
    print("\n[Step 2] éªŒè¯æ¨¡å‹åŠ è½½...")
    try:
        policy = PI0Policy.from_pretrained(MODEL_PATH)
        policy.to(device)
        policy.eval()
        print("âœ… Pi0 æ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"   - Config State Dim: {policy.config.max_state_dim}")
        print(f"   - Config Action Dim: {policy.config.max_action_dim}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # ---------------------------------------------------------
    # æ­¥éª¤ 3: å‡†å¤‡ Tokenizer (HuggingFace)
    # ---------------------------------------------------------
    print("\n[Step 3] å‡†å¤‡ Tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH) # é€šå¸¸åœ¨åŒä¸€ç›®å½•
        print("âœ… Tokenizer åŠ è½½æˆåŠŸ!")
    except Exception as e:
        print(f"âŒ Tokenizer åŠ è½½å¤±è´¥: {e}")
        return

    # ---------------------------------------------------------
    # æ­¥éª¤ 4: æ„é€ æ•°æ®å¹¶æ¨ç† (Integration Test)
    # ---------------------------------------------------------
    print("\n[Step 4] æ‰§è¡Œå®Œæ•´æ¨ç†æµç¨‹...")
    
    # 4.1 æ„é€ æ–‡æœ¬
    text = "Pick up the apple"
    tokens = tokenizer(text, return_tensors="pt", padding="max_length", max_length=48, truncation=True)
    
    # 4.2 æ„é€  Dummy å›¾åƒå’ŒçŠ¶æ€
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æ‚¨ä¹‹å‰æŠ¥é”™ä¿¡æ¯é‡Œç¡®è®¤çš„ Key åç§°
    dummy_image = torch.randn(1, 3, 224, 224).to(device)
    dummy_state = torch.randn(1, policy.config.max_state_dim).to(device)
    
    # 4.3 å½’ä¸€åŒ–è¾“å…¥çŠ¶æ€ (å¦‚æœæœ‰ Normalizer)
    if normalizer:
        # æ¨¡æ‹Ÿï¼šå‡è®¾ dummy_state æ˜¯çœŸå®ä¸–ç•Œçš„æ•°å€¼ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæŠŠå®ƒå½’ä¸€åŒ–å†å–‚ç»™æ¨¡å‹
        # å…¬å¼: (real - mean) / std
        model_input_state = normalizer.normalize(dummy_state, key="observation.state")
    else:
        model_input_state = dummy_state

    # 4.4 ç»„è£… Batch
    batch = {
        "observation.images.base_0_rgb": dummy_image,
        "observation.state": model_input_state,
        "observation.language_instruction.input_ids": tokens["input_ids"].to(device),
        "observation.language_instruction.attention_mask": tokens["attention_mask"].to(device),
    }

    # 4.5 æ¨¡å‹æ¨ç†
    try:
        with torch.no_grad():
            raw_action = policy.select_action(batch)
        print("âœ… æ¨¡å‹æ¨ç†æˆåŠŸ! è·å¾—åŸå§‹è¾“å‡º (Normalized Action)ã€‚")
    except Exception as e:
        print(f"âŒ æ¨ç†è¿‡ç¨‹å´©æºƒ: {e}")
        import traceback
        traceback.print_exc()
        return

    # ---------------------------------------------------------
    # æ­¥éª¤ 5: åå½’ä¸€åŒ–éªŒè¯ (Output Verification)
    # ---------------------------------------------------------
    print("\n[Step 5] éªŒè¯ç»“æœåå½’ä¸€åŒ–...")
    
    print(f"ğŸ¤– åŸå§‹è¾“å‡º (å‰4ä½): {raw_action[0, :4].cpu().numpy()}")
    
    if normalizer:
        try:
            # æ ¸å¿ƒæµ‹è¯•ï¼šæŠŠæ¨¡å‹è¾“å‡ºè¿˜åŸä¸ºçœŸå®ç‰©ç†é‡
            real_action = normalizer.denormalize(raw_action, key="action", mode="mean_std")
            
            print(f"ğŸ¦¾ çœŸå®åŠ¨ä½œ (å‰4ä½): {real_action[0, :4].cpu().numpy()}")
            print("\nğŸ‰ğŸ‰ğŸ‰ éªŒè¯é€šè¿‡ï¼æ‰€æœ‰æ¨¡å—å·¥ä½œæ­£å¸¸ï¼")
            print("æ‚¨çš„ Mini-Pi0 é¡¹ç›®ç°åœ¨å¯ä»¥ç‹¬ç«‹è¿è¡Œäº†ã€‚")
            
        except Exception as e:
            print(f"âŒ åå½’ä¸€åŒ–è®¡ç®—å¤±è´¥: {e}")
    else:
        print("âš ï¸  è·³è¿‡åå½’ä¸€åŒ–ï¼ˆå› ä¸ºæ²¡æœ‰åŠ è½½ Normalizerï¼‰")

if __name__ == "__main__":
    main()