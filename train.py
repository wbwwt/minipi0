import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, get_scheduler
from torch.optim import AdamW
import os
from pathlib import Path
from tqdm import tqdm
import logging
import json

# --- å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ---
from model.config import PI0Config 
from model.modeling_pi0 import PI0Policy
from data.dataset import Pi0Dataset
from utils.normalization import Normalizer

from pathlib import Path
current_dir = Path(__file__).parent
print(current_dir)

# ================= ğŸ”§ è®­ç»ƒé…ç½®åŒºåŸŸ =================
PRETRAINED_MODEL_PATH = "/root/Users/wangbo/pi0"
TOKENIZER_PATH = "/root/Users/wangbo/lerobot/tokenizers/paligemma"
DATASET_ROOT = "/root/Users/wangbo/my_converted_dataset"
STATS_PATH = "/root/Users/wangbo/my_converted_dataset/meta/stats.json" 
OUTPUT_DIR = "/root/Users/wangbo/minipi0/outputs/test"

BATCH_SIZE = 4       
LEARNING_RATE = 1e-4 
NUM_EPOCHS = 10      
SAVE_STEPS = 50      
LOG_STEPS = 1        
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ===================================================

# def save_checkpoint(model, tokenizer, config, output_dir):
#     """æ‰‹åŠ¨ä¿å­˜æ£€æŸ¥ç‚¹çš„è¾…åŠ©å‡½æ•°"""
#     os.makedirs(output_dir, exist_ok=True)
    
#     # 1. ä¿å­˜æƒé‡ (safetensors)
#     from safetensors.torch import save_file
#     save_file(model.state_dict(), os.path.join(output_dir, "model.safetensors"))
    
#     # 2. ä¿å­˜ Config
#     config.save_pretrained(output_dir)
    
#     # 3. ä¿å­˜ Tokenizer
#     tokenizer.save_pretrained(output_dir)
#     print(f"\nğŸ’¾ æ‰‹åŠ¨ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")

def save_checkpoint(model, tokenizer, config, output_dir):
    """æ‰‹åŠ¨ä¿å­˜æ£€æŸ¥ç‚¹çš„è¾…åŠ©å‡½æ•°"""
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. è·å–æ¨¡å‹çŠ¶æ€å­—å…¸
    state_dict = model.state_dict()
    
    # -----------------------------------------------------------
    # ğŸ”§ ä¿®å¤ safetensors å…±äº«å†…å­˜æŠ¥é”™ (Weight Tying Fix)
    # -----------------------------------------------------------
    # æŠ¥é”™æ˜¾ç¤ºçš„ä¸¤ä¸ª key å…±äº«äº†å†…å­˜ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå…¶ä¸­ä¸€ä¸ª clone æˆç‹¬ç«‹çš„
    problematic_key = "model.paligemma_with_expert.paligemma.lm_head.weight"
    
    if problematic_key in state_dict:
        # .clone() ä¼šåˆ›å»ºä¸€ä»½æ–°çš„æ•°æ®å‰¯æœ¬ï¼Œæ‰“ç ´å†…å­˜å…±äº«
        state_dict[problematic_key] = state_dict[problematic_key].clone()
    # -----------------------------------------------------------

    # 2. ä¿å­˜æƒé‡ (safetensors)
    from safetensors.torch import save_file
    save_file(state_dict, os.path.join(output_dir, "model.safetensors"))
    
    # 3. ä¿å­˜ Config
    config.save_pretrained(output_dir)
    
    # 4. ä¿å­˜ Tokenizer
    tokenizer.save_pretrained(output_dir)
    print(f"\nğŸ’¾ æ‰‹åŠ¨ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")

def main():
    logging.basicConfig(level=logging.INFO)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ! è®¾å¤‡: {DEVICE}")

    # 1. åŠ è½½ç»Ÿè®¡æ•°æ® (CPU)
    print(f"Loading stats from {STATS_PATH}...")
    normalizer = Normalizer(STATS_PATH, device="cpu")
    
    if 'action' in normalizer.stats:
        action_dim = normalizer.stats['action']['mean'].shape[0]
        state_dim = normalizer.stats['observation.state']['mean'].shape[0]
        print(f"ğŸ“ æ£€æµ‹åˆ°æ•°æ®ç»´åº¦: Action={action_dim}, State={state_dim}")
    else:
        raise ValueError("ç»Ÿè®¡æ–‡ä»¶ä¸­ç¼ºå°‘ 'action' å­—æ®µï¼")

    # 2. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)

    # 3. å‡†å¤‡æ•°æ®é›†
    print("ğŸ“š åŠ è½½æ•°æ®é›†...")
    train_dataset = Pi0Dataset(
        root_dir=DATASET_ROOT,
        tokenizer=tokenizer,
        normalizer=normalizer,
        split="train",
        image_size=224,
        action_chunk_size=50
    )
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        num_workers=0,  
        drop_last=True
    )
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(train_dataset)} ä¸ªæ ·æœ¬")

    # 4. åŠ è½½å¹¶æ”¹é€ æ¨¡å‹
    print("ğŸ§  åŠ è½½æ¨¡å‹ä¸­...")
    config = PI0Config.from_pretrained(PRETRAINED_MODEL_PATH)
    
    print(f"ğŸ”„ ä¿®æ”¹æ¨¡å‹é…ç½®: Action Dim {config.max_action_dim} -> {action_dim}")
    config.max_state_dim = state_dim
    config.max_action_dim = action_dim
    
    # ä½¿ç”¨æˆ‘ä»¬æ‰‹å†™çš„å…¼å®¹ç‰ˆ from_pretrained
    policy = PI0Policy.from_pretrained(
        PRETRAINED_MODEL_PATH, 
        config=config, 
        ignore_mismatched_sizes=True
    )
    policy.to(DEVICE)
    policy.train()

    # 5. ä¼˜åŒ–å™¨
    optimizer = AdamW(policy.parameters(), lr=LEARNING_RATE)
    num_training_steps = NUM_EPOCHS * len(train_loader)
    lr_scheduler = get_scheduler(
        "cosine",
        optimizer=optimizer,
        num_warmup_steps=min(50, num_training_steps // 10), 
        num_training_steps=num_training_steps
    )

    # 6. è®­ç»ƒå¾ªç¯
    print("ğŸ”¥ å¼€å§‹å¾ªç¯å¾®è°ƒ...")
    global_step = 0
    
    for epoch in range(NUM_EPOCHS):
        print(f"\n=== Epoch {epoch+1}/{NUM_EPOCHS} ===")
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        
        for batch in progress_bar:
            for k, v in batch.items():
                if isinstance(v, torch.Tensor):
                    batch[k] = v.to(DEVICE)
            
            # --- å‰å‘ä¼ æ’­ ---
            # å› ä¸ºæˆ‘ä»¬çš„ forward ç›´æ¥è¿”å› lossï¼Œä¸éœ€è¦åˆ¤æ–­å­—å…¸äº†
            loss = policy(batch) 
            
            # --- åå‘ä¼ æ’­ ---
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

            global_step += 1
            if global_step % LOG_STEPS == 0:
                progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
                
            # --- ä¿å­˜ ---
            if global_step % SAVE_STEPS == 0:
                save_path = os.path.join(OUTPUT_DIR, f"checkpoint-{global_step}")
                # ä½¿ç”¨è¾…åŠ©å‡½æ•°æ‰‹åŠ¨ä¿å­˜
                save_checkpoint(policy, tokenizer, config, save_path)
                
    # 7. æœ€ç»ˆä¿å­˜
    final_path = os.path.join(OUTPUT_DIR, "final_model")
    save_checkpoint(policy, tokenizer, config, final_path)
    print(f"\nğŸ‰ è®­ç»ƒç»“æŸï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜åœ¨: {final_path}")

if __name__ == "__main__":
    main()