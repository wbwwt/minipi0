import sys
import os
import time
import torch
import numpy as np
import cv2
from transformers import AutoTokenizer
from transformers import BitsAndBytesConfig

# ==================== 0. è·¯å¾„ä¸ä¾èµ–é…ç½® ====================
# å¿…é¡»å…ˆæ‰§è¡Œè¿™ä¸€æ­¥ï¼Œç¡®ä¿ Python èƒ½æ‰¾åˆ° startouch é©±åŠ¨
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
STARTOUCH_PATH = os.path.join(CURRENT_DIR, 'startouch-v1', 'interface_py')

if STARTOUCH_PATH not in sys.path:
    print(f"ğŸ”§ å°†é©±åŠ¨è·¯å¾„åŠ å…¥ç³»ç»Ÿ Path: {STARTOUCH_PATH}")
    sys.path.append(STARTOUCH_PATH)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from utils.camera import RealSenseCamera
from model.config import PI0Config
from model.modeling_pi0 import PI0Policy
from utils.normalization import Normalizer

# ==================== ğŸ”§ é…ç½®åŒºåŸŸ ====================
# æ¨¡å‹è·¯å¾„
MODEL_PATH = "/home/lumos/pi0/weight/checkpoint-28000"
STATS_PATH = "/home/lumos/pi0/replay_remote_ctrl/my_converted_dataset_v3/meta/stats.json"
TOKENIZER_PATH = "/home/lumos/pi0/replay_remote_ctrl/minipi0/paligemma" # è¯·ä¿®æ”¹

# ä»»åŠ¡æè¿°
TASK_DESC = "pick up the cube"

# ç¡¬ä»¶é…ç½®
CAN_INTERFACE = "can0"     # Startouch é»˜è®¤ CAN å£
CONTROL_HZ = 30            # æ§åˆ¶é¢‘ç‡
DT = 1.0 / CONTROL_HZ
IMAGE_SIZE = 224
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ===================================================

def setup_robot(can_interface="can0", enable_gripper=True):
    """
    åˆå§‹åŒ– Startouch æœºæ¢°è‡‚
    """
    print(f"ğŸ¤– æ­£åœ¨åˆå§‹åŒ– Startouch æœºæ¢°è‡‚ (CAN: {can_interface})...")
    
    try:
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé˜²æ­¢å› ç¼ºå°‘ .so æ–‡ä»¶å¯¼è‡´è„šæœ¬ç›´æ¥å´©æºƒæ— æ³•æ•è·å¼‚å¸¸
        sys.path.append('/home/lumos/pi0/replay_remote_ctrl/startouch-v1/interface_py')
        # from startouchclass import SingleArm
        from startouchclass import SingleArm
        
        # åˆå§‹åŒ–æœºæ¢°è‡‚å¯¹è±¡
        # æ³¨æ„ï¼šSingleArm å†…éƒ¨ä¼šè‡ªåŠ¨åŠ è½½ param_csv_gripper ä¸‹çš„å‚æ•°æ–‡ä»¶
        # åªè¦ä¿æŒç›®å½•ç»“æ„ä¸å˜ (minipi0/startouch-v1/...)ï¼Œå®ƒå°±èƒ½æ‰¾åˆ°
        robot = SingleArm(can_interface_=can_interface, gripper=enable_gripper)
        robot.set_joint([-0.01468681 , 0.58384833 , 0.23212787 , 0.38242924 ,-0.04100862 , 0.        ] , tf=3)

        
        print("âœ… Startouch æœºæ¢°è‡‚è¿æ¥æˆåŠŸ")
        
        # å¯é€‰ï¼šè¿›è¡Œä¸€äº›è‡ªæ£€æˆ–å½’ä½
        # print("   æ­£åœ¨å½’ä½ (Home)...")
        # robot.go_home()
        
        return robot
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥ Startouch é©±åŠ¨: {e}")
        print(f"   è¯·æ£€æŸ¥ {STARTOUCH_PATH} ä¸‹æ˜¯å¦å­˜åœ¨ .so æ–‡ä»¶")
        raise e
    except Exception as e:
        print(f"âŒ æœºæ¢°è‡‚åˆå§‹åŒ–å¤±è´¥: {e}")
        # è¿™é‡Œå¯èƒ½éœ€è¦æ£€æŸ¥ CAN å¡æ˜¯å¦å¯ç”¨: sudo ip link set can0 up type can bitrate 1000000
        raise e

def main():
    print(f"ğŸš€ å¯åŠ¨å®æœºæ¨ç†... è®¾å¤‡: {DEVICE}")

    # ---------------------------------------------------------
    # 1. ç¡¬ä»¶åˆå§‹åŒ– (Robot & Camera)
    # ---------------------------------------------------------
    try:
        robot = setup_robot(can_interface=CAN_INTERFACE, enable_gripper=True)
    except Exception:
        return # åˆå§‹åŒ–å¤±è´¥ç›´æ¥é€€å‡º

    print("ğŸ“· æ­£åœ¨æ‰“å¼€ RealSense æ‘„åƒå¤´...")
    try:
        camera = RealSenseCamera(width=640, height=480, fps=30)
    except Exception as e:
        print(f"âŒ æ‘„åƒå¤´æ‰“å¼€å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿å·²è¿æ¥ USB3.0 æ¥å£å¹¶å®‰è£…äº† pyrealsense2")
        return

    # ---------------------------------------------------------
    # 2. æ¨¡å‹åŠ è½½
    # ---------------------------------------------------------
    print("ğŸ§  åŠ è½½ Pi0 æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)
    
    config = PI0Config.from_pretrained(MODEL_PATH)
    config.max_action_dim = 8
    config.max_state_dim = 8

    # ğŸ”§ å®šä¹‰ 4-bit é‡åŒ–é…ç½®
    # quantization_config = BitsAndBytesConfig(
    #     load_in_4bit=True,
    #     bnb_4bit_compute_dtype=torch.bfloat16, # è®¡ç®—æ—¶ç”¨ bf16
    #     bnb_4bit_quant_type="nf4",             # ä½¿ç”¨ nf4 æ ¼å¼ç²¾åº¦æ›´é«˜
    #     bnb_4bit_use_double_quant=True         # äºŒæ¬¡é‡åŒ–ï¼Œè¿›ä¸€æ­¥çœæ˜¾å­˜
    # )

    # ğŸ”§ å®šä¹‰ 4-bit é‡åŒ–é…ç½® (æ”¹ç”¨ float16)
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        # bnb_4bit_compute_dtype=torch.float16, # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œï¼šç”¨ float16
        bnb_4bit_quant_type="nf4",             
        bnb_4bit_use_double_quant=True         
    )

    policy = PI0Policy.from_pretrained(
        MODEL_PATH, 
        config=config,
        # torch_dtype=torch.bfloat16,
        quantization_config=quantization_config,
        low_cpu_mem_usage=True
    )
    # policy = PI0Policy.from_pretrained(MODEL_PATH, config=config)
    # policy.to(DEVICE)
    # policy.to(device=DEVICE, dtype=torch.bfloat16)
    policy.eval()

    print(f"ğŸ“Š åŠ è½½ç»Ÿè®¡æ•°æ®: {STATS_PATH}")
    # normalizer = Normalizer(STATS_PATH, device=DEVICE)
    normalizer = Normalizer(STATS_PATH, device='cpu')
    # é¢„å¤„ç†æ–‡æœ¬
    text_tokens = tokenizer(TASK_DESC, return_tensors="pt", padding="max_length", max_length=48, truncation=True)
    input_ids = text_tokens["input_ids"].to(DEVICE)
    attention_mask = text_tokens["attention_mask"].to(DEVICE)

    print("\nâœ… ç³»ç»Ÿå°±ç»ª! æ­¤æ—¶è¯·ç¡®ä¿æœºæ¢°è‡‚å‘¨å›´å®‰å…¨ã€‚")
    print("ğŸ‘‰ æŒ‰ä¸‹ Enter é”®å¼€å§‹æ‰§è¡Œæ¨ç†...")
    input()

    # ---------------------------------------------------------
    # 3. æ§åˆ¶å¾ªç¯
    # ---------------------------------------------------------
    print(f"ğŸ”¥ å¼€å§‹æ‰§è¡Œå¾ªç¯ ({CONTROL_HZ} Hz)... æŒ‰ Ctrl+C åœæ­¢")
    
    try:
        step = 0
        while True:
            t_start = time.time()

            # --- A. è·å–ä¼ æ„Ÿå™¨æ•°æ® ---
            img_bgr = camera.get_frame()
            if img_bgr is None:
                print("âš ï¸ ä¸¢å¸§ (Camera)")
                continue
            
            # è·å–æœºæ¢°è‡‚çŠ¶æ€ [j1...j6, gripper]
            # joints = robot.get_joint_positions()   # np.array (6,)
            # joints = robot.get_ee_pose_quat()
            # gripper = robot.get_gripper_position() # float
            # current_state = np.append(joints, gripper).astype(np.float32)
            
            pos_t, quat_t = robot.get_ee_pose_quat() 
            # pos_t: [x,y,z], quat_t: [w,x,y,z] (æ ¹æ®ä½ çš„æè¿°)
            
            # 2. è·å–å¤¹çˆª
            gripper = robot.get_gripper_position()
            
            # 3. æ‹¼æ¥ [x,y,z, w,x,y,z, gripper] (å…±8ç»´)
            # âš ï¸ æå…¶é‡è¦: ç¡®è®¤ä½ è®­ç»ƒæ—¶çš„å››å…ƒæ•°é¡ºåºæ˜¯ wxyz è¿˜æ˜¯ xyzwï¼Ÿ
            # å¦‚æœè®­ç»ƒé›†æ˜¯ [x,y,z,qx,qy,qz,qw]ï¼Œä½ éœ€è¦æŠŠ quat_t[0] (w) æŒªåˆ°æœ€å
            # å‡è®¾è®­ç»ƒé›†é€šå¸¸æ˜¯ [x,y,z, qx,qy,qz,qw]ï¼Œå¦‚ä¸‹è°ƒæ•´ï¼š
            # quat_reordered = np.array([quat_t[1], quat_t[2], quat_t[3], quat_t[0]]) 
            # è¿™é‡Œå…ˆæŒ‰ä½ çš„åŸå§‹è¾“å‡ºæ‹¼æ¥ï¼Œè¯·åŠ¡å¿…æ ¸å¯¹ï¼
            
            current_state = np.concatenate([pos_t, quat_t, [gripper]]).astype(np.float32)

            # # --- B. æ•°æ®é¢„å¤„ç† ---
            # # å›¾åƒ: BGR->RGB, Resize, Normalize
            # img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
            # if img_rgb.shape[0] != IMAGE_SIZE or img_rgb.shape[1] != IMAGE_SIZE:
            #      img_resized = cv2.resize(img_rgb, (IMAGE_SIZE, IMAGE_SIZE))
            # else:
            #      img_resized = img_rgb
                 
            # img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0
            # img_tensor = img_tensor.unsqueeze(0).to(DEVICE) 

            # # çŠ¶æ€: Normalize
            # state_tensor = torch.from_numpy(current_state).unsqueeze(0).to(DEVICE) 
            # state_norm = normalizer.normalize(state_tensor, key="observation.state")

            # # --- C. æ¨¡å‹æ¨ç† ---
            # batch = {
            #     "observation.images.base_0_rgb": img_tensor,
            #     "observation.state": state_norm,
            #     "observation.language_instruction.input_ids": input_ids,
            #     "observation.language_instruction.attention_mask": attention_mask
            # }

            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
            if img_rgb.shape[0] != IMAGE_SIZE or img_rgb.shape[1] != IMAGE_SIZE:
                 img_resized = cv2.resize(img_rgb, (IMAGE_SIZE, IMAGE_SIZE))
            else:
                 img_resized = img_rgb
            
            img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0
            # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œï¼šç”¨ float16
            # img_tensor = img_tensor.unsqueeze(0).to(DEVICE, dtype=torch.float16)
            img_tensor = img_tensor.unsqueeze(0).to(DEVICE)

            # çŠ¶æ€å¤„ç† (CPU Float32 è®¡ç®— -> GPU Float16)
            state_tensor_cpu = torch.from_numpy(current_state).float().unsqueeze(0) 
            
            stats_mean = normalizer.stats["observation.state"]["mean"].cpu()
            stats_std = normalizer.stats["observation.state"]["std"].cpu()
            
            state_norm_cpu = (state_tensor_cpu - stats_mean) / (stats_std + 1e-8)
            
            # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œï¼šç”¨ float16
            # state_norm = state_norm_cpu.to(DEVICE, dtype=torch.float16)
            state_norm = state_norm_cpu.to(DEVICE)

            # --- C. æ¨¡å‹æ¨ç† ---
            batch = {
                "observation.images.base_0_rgb": img_tensor,
                "observation.state": state_norm,
                "observation.language_instruction.input_ids": input_ids,
                "observation.language_instruction.attention_mask": attention_mask
            }

            with torch.no_grad():
                # Action Chunking é€»è¾‘å°è£…åœ¨ select_action å†…
                action_norm = policy.select_action(batch)

            # ***********************************
            action_norm = action_norm.cpu()

            # --- D. æ‰§è¡ŒåŠ¨ä½œ ---
            action_real = normalizer.denormalize(action_norm, key="action")
            action_np = action_real.squeeze(0).cpu().numpy()

            # è§£æåŠ¨ä½œ
            target_joints = action_np[:-1] # å‰7ä¸ª
            target_gripper = action_np[-1] # æœ€å1ä¸ª

            # å‘é€æŒ‡ä»¤ (é€ä¼ )
            # é€Ÿåº¦è®¾ä¸º0ï¼Œç”±åº•å±‚æ§åˆ¶å™¨è´Ÿè´£æ’è¡¥
            # robot.set_joint_raw(target_joints, velocities=[0.0]*6)
            robot.set_end_effector_pose_quat_raw(target_joints[:3], target_joints[3:])
            robot.setGripperPosition_raw(target_gripper)

            # --- E. é¢‘ç‡æ§åˆ¶ ---
            elapsed = time.time() - t_start
            sleep_time = DT - elapsed
            if sleep_time > 0:
                time.sleep(sleep_time)
            
            # çŠ¶æ€ç›‘æ§äº† bnb_4bit_compute_dtype=torch.floa
            if step % 30 == 0:
                print(f"Step {step} | Gripper: {gripper:.2f}->{target_gripper:.2f}")
            step += 1

    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·åœæ­¢")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("èµ„æºæ¸…ç†ä¸­...")
        if 'camera' in locals(): camera.stop()
        # robot å¯¹è±¡é€šå¸¸ä¸éœ€è¦æ˜¾å¼ closeï¼Œææ„æ—¶ä¼šè‡ªåŠ¨é‡Šæ”¾
        # å¦‚æœéœ€è¦å›é›¶: robot.go_home()

if __name__ == "__main__":
    main()