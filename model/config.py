# import dataclasses
# from dataclasses import dataclass, field
# from typing import Dict, List, Optional, Tuple, Any

# from transformers import PretrainedConfig

# # --- å¸¸é‡å®šä¹‰ ---
# DEFAULT_IMAGE_SIZE = 224

# @dataclass
# class RTCConfig:
#     """å®æ—¶æ§åˆ¶é…ç½® (ä¿ç•™æ­¤ç±»ä»¥å…åŠ è½½æ—§ Config æŠ¥é”™ï¼Œè™½æš‚ä¸å¯ç”¨)"""
#     enabled: bool = False
#     name: str = "frequency_pd"
#     P: float = 0.05
#     D: float = 0.005
#     target_frequency: float = 15.0

# @dataclass
# class PI0Config(PretrainedConfig):
#     model_type = "pi0"
    
#     # --- 1. åŸºç¡€æ¨¡å‹é…ç½® ---
#     paligemma_variant: str = "gemma_300m"
#     action_expert_variant: str = "gemma_300m"
    
#     # å›¾åƒåˆ†è¾¨ç‡ (Height, Width)
#     image_resolution: Tuple[int, int] = (DEFAULT_IMAGE_SIZE, DEFAULT_IMAGE_SIZE)
    
#     # --- 2. æ ¸å¿ƒç»´åº¦é…ç½® (å¿…é¡»ä¸è®­ç»ƒæ•°æ®ä¸€è‡´) ---
#     # çŠ¶æ€ç»´åº¦ (ä½ çš„æœºæ¢°è‡‚å…³èŠ‚æ•° + å¤¹çˆª)
#     max_state_dim: int = 7  # ä¾‹å¦‚: 6å…³èŠ‚ + 1å¤¹çˆª
#     # åŠ¨ä½œç»´åº¦
#     max_action_dim: int = 7 
    
#     # --- 3. åºåˆ—ä¸æ—¶é—´ç›¸å…³ ---
#     # æ¯æ¬¡é¢„æµ‹å¤šå°‘æ­¥ (Chunk Size)
#     chunk_size: int = 50
#     # æ¨ç†æ—¶å®é™…æ‰§è¡Œå¤šå°‘æ­¥
#     n_action_steps: int = 10  # é€šå¸¸å°äº chunk_size
    
#     # æ‰©æ•£/æµåŒ¹é…å‚æ•°
#     num_inference_steps: int = 10
#     min_period: float = 0.01
#     max_period: float = 1000.0
    
#     # æ—¶é—´é‡‡æ ·å‚æ•° (Training only)
#     time_sampling_beta_alpha: float = 1.0
#     time_sampling_beta_beta: float = 1.0
#     time_sampling_scale: float = 1.0
#     time_sampling_offset: float = 0.0
    
#     # --- 4. è®­ç»ƒç­–ç•¥å‚æ•° ---
#     freeze_vision_encoder: bool = False
#     train_expert_only: bool = False
#     gradient_checkpointing: bool = False
#     dtype: str = "bfloat16" # "float32" or "bfloat16"
    
#     # --- 5. ç‰¹å¾æè¿° (ç”¨äºè‡ªåŠ¨æ¨æ–­ç»´åº¦ï¼Œè¿™é‡Œç•™ç©ºæˆ–æ‰‹åŠ¨æŒ‡å®š) ---
#     # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ input_featuresï¼Œä¸å†ä¾èµ– LeRobot çš„å¤æ‚ schema
#     input_features: Dict[str, Any] = field(default_factory=lambda: {
#         "observation.images.cam_high": {"shape": (3, 224, 224), "dtype": "float32"},
#         "observation.state": {"shape": (7,), "dtype": "float32"},
#         "observation.language_instruction": {"shape": (1,), "dtype": "string"}
#     })
    
#     output_features: Dict[str, Any] = field(default_factory=lambda: {
#         "action": {"shape": (7,), "dtype": "float32"}
#     })
    
#     # --- 6. å½’ä¸€åŒ–æ˜ å°„ (æå…¶é‡è¦ï¼Œç”¨äºæ¨ç†æ—¶åå½’ä¸€åŒ–) ---
#     # è¿™é‡Œå­˜å‚¨çš„æ˜¯ key åˆ° normalization mode çš„æ˜ å°„
#     normalization_mapping: Dict[str, str] = field(default_factory=lambda: {
#         "observation.images.cam_high": "identity", # å›¾åƒé€šå¸¸ç”± transform å¤„ç†
#         "observation.state": "mean_std",
#         "action": "mean_std",
#     })
    
#     # --- 7. RTC é…ç½® ---
#     rtc_config: Optional[RTCConfig] = None
    
#     # --- 8. ç¼–è¯‘é€‰é¡¹ ---
#     compile_model: bool = False
#     compile_mode: str = "reduce-overhead"
    
#     # å¿…é¡»çš„åˆå§‹åŒ–å‡½æ•°ï¼Œç”¨äºæ¥æ”¶ **kwargs
#     def __init__(self, **kwargs):
#         # æå– input_features å’Œ output_features ä¸­å¯èƒ½å­˜åœ¨çš„ç»´åº¦ä¿¡æ¯
#         # ä»¥è¦†ç›– max_state_dim å’Œ max_action_dim
#         if "input_features" in kwargs:
#             feats = kwargs["input_features"]
#             if "observation.state" in feats:
#                 self.max_state_dim = feats["observation.state"]["shape"][0]
        
#         if "output_features" in kwargs:
#             feats = kwargs["output_features"]
#             if "action" in feats:
#                 self.max_action_dim = feats["action"]["shape"][0]
                
#         # å¤„ç† rtc_config ä» dict è½¬ä¸ºå¯¹è±¡ (å¦‚æœä» json åŠ è½½)
#         if "rtc_config" in kwargs and isinstance(kwargs["rtc_config"], dict):
#             kwargs["rtc_config"] = RTCConfig(**kwargs["rtc_config"])

#         super().__init__(**kwargs)

#     # å…¼å®¹æ€§å‡½æ•°ï¼šæ¨¡ä»¿ LeRobot çš„ validate_featuresï¼Œè¿™é‡Œä»€ä¹ˆéƒ½ä¸åšæˆ–ä»…åšç®€å•æ£€æŸ¥
#     def validate_features(self):
#         pass


from typing import Dict, List, Optional, Tuple, Any
from transformers import PretrainedConfig

# --- å¸¸é‡å®šä¹‰ ---
DEFAULT_IMAGE_SIZE = 224

class PI0Config(PretrainedConfig):
    model_type = "pi0"
    
    def __init__(
        self,
        # 1. åŸºç¡€æ¨¡å‹é…ç½®
        paligemma_variant: str = "gemma_300m",
        action_expert_variant: str = "gemma_300m",
        image_resolution: Tuple[int, int] = (DEFAULT_IMAGE_SIZE, DEFAULT_IMAGE_SIZE),
        
        # 2. æ ¸å¿ƒç»´åº¦é…ç½®
        max_state_dim: int = 7,
        max_action_dim: int = 7,
        
        # 3. åºåˆ—ä¸æ—¶é—´ç›¸å…³
        chunk_size: int = 50,
        n_action_steps: int = 10,
        
        # æ‰©æ•£/æµåŒ¹é…å‚æ•°
        num_inference_steps: int = 10,
        min_period: float = 0.01,
        max_period: float = 1000.0,
        
        # æ—¶é—´é‡‡æ ·å‚æ•°
        time_sampling_beta_alpha: float = 1.0,
        time_sampling_beta_beta: float = 1.0,
        time_sampling_scale: float = 1.0,
        time_sampling_offset: float = 0.0,
        
        # 4. è®­ç»ƒç­–ç•¥å‚æ•°
        freeze_vision_encoder: bool = False,
        train_expert_only: bool = False,
        gradient_checkpointing: bool = False,
        dtype: str = "bfloat16",
        
        # 5. ç‰¹å¾æè¿° (é»˜è®¤ä¸º Noneï¼Œåœ¨ init é‡Œåˆå§‹åŒ–)
        input_features: Optional[Dict[str, Any]] = None,
        output_features: Optional[Dict[str, Any]] = None,
        normalization_mapping: Optional[Dict[str, str]] = None,
        
        # 6. RTC é…ç½®
        rtc_config: Optional[Dict[str, Any]] = None,
        
        # 7. ç¼–è¯‘é€‰é¡¹
        compile_model: bool = False,
        compile_mode: str = "reduce-overhead",
        
        **kwargs
    ):
        self.paligemma_variant = paligemma_variant
        self.action_expert_variant = action_expert_variant
        self.image_resolution = image_resolution
        self.max_state_dim = max_state_dim
        self.max_action_dim = max_action_dim
        self.chunk_size = chunk_size
        self.n_action_steps = n_action_steps
        self.num_inference_steps = num_inference_steps
        self.min_period = min_period
        self.max_period = max_period
        self.time_sampling_beta_alpha = time_sampling_beta_alpha
        self.time_sampling_beta_beta = time_sampling_beta_beta
        self.time_sampling_scale = time_sampling_scale
        self.time_sampling_offset = time_sampling_offset
        self.freeze_vision_encoder = freeze_vision_encoder
        self.train_expert_only = train_expert_only
        self.gradient_checkpointing = gradient_checkpointing
        self.dtype = dtype
        self.compile_model = compile_model
        self.compile_mode = compile_mode

        # å¤„ç† Mutable Defaults (å­—å…¸ä¸èƒ½åšé»˜è®¤å‚æ•°ï¼Œå¿…é¡»åœ¨ init é‡Œèµ‹å€¼)
        if input_features is None:
            self.input_features = {
                "observation.images.cam_high": {"shape": (3, 224, 224), "dtype": "float32"},
                "observation.state": {"shape": (max_state_dim,), "dtype": "float32"},
                "observation.language_instruction": {"shape": (1,), "dtype": "string"}
            }
        else:
            self.input_features = input_features

        if output_features is None:
            self.output_features = {
                "action": {"shape": (max_action_dim,), "dtype": "float32"}
            }
        else:
            self.output_features = output_features

        # ğŸ‘‡ å…³é”®ä¿®å¤ï¼šç¡®ä¿ normalization_mapping ä¸€å®šè¢«èµ‹å€¼
        if normalization_mapping is None:
            self.normalization_mapping = {
                "observation.images.cam_high": "identity",
                "observation.state": "mean_std",
                "action": "mean_std",
            }
        else:
            self.normalization_mapping = normalization_mapping

        self.rtc_config = rtc_config

        # åŠ¨æ€è°ƒæ•´ç»´åº¦
        if "observation.state" in self.input_features:
            self.max_state_dim = self.input_features["observation.state"]["shape"][0]
        if "action" in self.output_features:
            self.max_action_dim = self.output_features["action"]["shape"][0]

        super().__init__(**kwargs)