import torch
import os
from pathlib import Path
from transformers import AutoTokenizer

# å¼•å…¥æˆ‘ä»¬åˆšæ‰æå–çš„æ¨¡å‹ç±»
from model.modeling_pi0 import PI0Policy

# ================= é…ç½®åŒºåŸŸ =================
# è¯·ä¿®æ”¹ä¸ºæ‚¨è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (åŒ…å« config.json, model.safetensors çš„æ–‡ä»¶å¤¹)
# ä¾‹å¦‚: "outputs/train/pi0_test" æˆ–è€…æ‚¨çš„é¢„è®­ç»ƒæƒé‡ç›®å½•
MODEL_PATH = "/root/Users/wangbo/pi0"  
# å¦‚æœæ‚¨çš„åˆ†è¯å™¨åœ¨å¦ä¸€ä¸ªç›®å½•ï¼Œè¯·ä¿®æ”¹è¿™é‡Œï¼›å¦‚æœåœ¨åŒä¸€ä¸ªç›®å½•ï¼Œä¿æŒ MODEL_PATH å³å¯
TOKENIZER_PATH = "/root/Users/wangbo/lerobot/tokenizers/paligemma" 
# ===========================================

def main():
    print(f"ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹ï¼Œè·¯å¾„: {MODEL_PATH}")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âš™ï¸  è¿è¡Œè®¾å¤‡: {device}")

    # 1. åŠ è½½æ¨¡å‹ (æµ‹è¯•æå–çš„ from_pretrained é€»è¾‘)
    try:
        policy = PI0Policy.from_pretrained(MODEL_PATH)
        policy.to(device)
        policy.eval() # åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # æ‰“å°ä¸€ä¸‹å…³é”®é…ç½®ï¼Œç¡®è®¤ config.py å·¥ä½œæ­£å¸¸
        print(f"   - State Dim: {policy.config.max_state_dim}")
        print(f"   - Action Dim: {policy.config.max_action_dim}")
        print(f"   - Image Size: {policy.config.image_resolution}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 2. åŠ è½½åˆ†è¯å™¨ (Tokenizer)
    # Pi0 å¿…é¡»è¦æœ‰åˆ†è¯å™¨æ‰èƒ½å¤„ç†æ–‡æœ¬æŒ‡ä»¤
    try:
        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥ (è¯·æ£€æŸ¥è·¯å¾„): {e}")
        return

    # 3. æ„é€ ä¼ªé€ è¾“å…¥æ•°æ® (Dummy Data)
    print("\nğŸ“¦ æ­£åœ¨æ„é€ æµ‹è¯•æ•°æ®...")
    
    # [A] æ–‡æœ¬æŒ‡ä»¤
    instruction = "Pick up the blue cube"
    tokenized = tokenizer(
        instruction, 
        return_tensors="pt", 
        padding="max_length", 
        max_length=48, # è¿™é‡Œçš„é•¿åº¦é€šå¸¸åœ¨ config é‡Œï¼Œè¿™é‡Œæš‚æ—¶å†™æ­»æµ‹è¯•
        truncation=True
    )
    
    # [B] å›¾åƒæ•°æ® (B, C, H, W)
    # æ¨¡æ‹Ÿä¸€å¼ éšæœºå™ªç‚¹çš„å›¾ç‰‡
    dummy_image = torch.rand(1, 3, 224, 224).to(device)
    
    # [C] æœºæ¢°è‡‚çŠ¶æ€ (B, State_Dim)
    # æ¨¡æ‹Ÿå½“å‰æœºæ¢°è‡‚ä½ç½®
    state_dim = policy.config.max_state_dim
    dummy_state = torch.randn(1, state_dim).to(device)

    # [D] ç»„è£…æˆ Batch å­—å…¸
    #è¿™æ˜¯ modeling_pi0.py é‡Œ forward/select_action æœŸå¾…çš„æ ¼å¼
    batch = {
        # å›¾åƒ Key (ä¿æŒä¸å˜)
        "observation.images.base_0_rgb": dummy_image, 
        
        # çŠ¶æ€ Key (ä¿æŒä¸å˜)
        "observation.state": dummy_state,
        
        # ğŸ”´ ä¿®æ”¹è¿™é‡Œï¼šæ”¹å›æŠ¥é”™ä¿¡æ¯é‡Œè¦æ±‚çš„â€œé•¿åå­—â€
        "observation.language_instruction.input_ids": tokenized["input_ids"].to(device),
        "observation.language_instruction.attention_mask": tokenized["attention_mask"].to(device),
    }

    # 4. æ‰§è¡Œæ¨ç†
    print("âš¡ å¼€å§‹æ‰§è¡Œæ¨ç† (select_action)...")
    try:
        with torch.no_grad():
            # select_action ä¼šè¿”å›ä¸‹ä¸€æ­¥åŠ¨ä½œ
            action = policy.select_action(batch)
        
        print("\nğŸ‰ æ¨ç†æˆåŠŸï¼")
        print(f"ğŸ“ è¾“å…¥æŒ‡ä»¤: '{instruction}'")
        print(f"ğŸ¤– è¾“å‡ºåŠ¨ä½œå½¢çŠ¶: {action.shape}")
        print(f"ğŸ“Š è¾“å‡ºåŠ¨ä½œæ•°å€¼ (å‰5ä½): {action[0, :5].cpu().numpy()}...")
        
        # éªŒè¯å½¢çŠ¶æ˜¯å¦ç¬¦åˆé¢„æœŸ: [Action_Dim]
        expected_dim = policy.config.max_action_dim
        if action.shape[-1] == expected_dim:
            print("âœ… è¾“å‡ºç»´åº¦éªŒè¯é€šè¿‡ã€‚")
        else:
            print(f"âš ï¸  è­¦å‘Š: è¾“å‡ºç»´åº¦ {action.shape[-1]} ä¸é…ç½® {expected_dim} ä¸ä¸€è‡´")

    except Exception as e:
        print(f"âŒ æ¨ç†è¿‡ç¨‹ä¸­å´©æºƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()