# import torch
# from torch.utils.data import Dataset
# import pandas as pd
# from pathlib import Path
# import numpy as np
# import logging
# from typing import Optional

# # ÊõøÊç¢ decord ‰∏∫ opencv
# try:
#     import cv2
# except ImportError:
#     raise ImportError("ËØ∑ÂÆâË£Ö opencv: pip install opencv-python-headless")

# from transformers import PreTrainedTokenizer
# from utils.normalization import Normalizer

# class Pi0Dataset(Dataset):
#     def __init__(
#         self, 
#         root_dir: str, 
#         tokenizer: PreTrainedTokenizer,
#         normalizer: Optional[Normalizer] = None,
#         split: str = "train",
#         image_size: int = 224,
#         max_token_len: int = 48,
#         action_chunk_size: int = 50,
#         video_key: str = "observation.images.cam_high",
#     ):
#         self.root_dir = Path(root_dir)
#         self.tokenizer = tokenizer
#         self.normalizer = normalizer
#         self.max_token_len = max_token_len
#         self.action_chunk_size = action_chunk_size
#         self.video_key = video_key
#         self.image_size = image_size
        
#         logging.info(f"Dataset root: {self.root_dir}")

#         # -----------------------------------------------------------
#         # 1. Âä†ËΩΩÂÖÉÊï∞ÊçÆ
#         # -----------------------------------------------------------
#         meta_dir = self.root_dir / "meta/episodes"
#         if not meta_dir.exists():
#             raise FileNotFoundError(f"Êâæ‰∏çÂà∞ meta/episodes ÁõÆÂΩï: {meta_dir}")
            
#         parquet_files = sorted(list(meta_dir.rglob("*.parquet")))
#         if not parquet_files:
#             raise FileNotFoundError(f"meta/episodes ‰∏ã‰∏∫Á©∫ÔºÅ")

#         print(f"‚úÖ ÂèëÁé∞ {len(parquet_files)} ‰∏™ÂÖÉÊï∞ÊçÆÊñá‰ª∂")
        
#         dfs = [pd.read_parquet(p) for p in parquet_files]
#         self.episodes = pd.concat(dfs, ignore_index=True)
        
#         # ‰øùÊä§ÈÄªËæëÔºöÂ¶ÇÊûúÂè™Êúâ‰∏Ä‰∏™ EpisodeÔºåÂÖ®Áî®
#         total_episodes = len(self.episodes)
#         if total_episodes == 1:
#             pass # ‰∏çÂàáÂàÜ
#         else:
#             train_len = int(total_episodes * 0.95)
#             if train_len == 0 and total_episodes > 0: train_len = 1
            
#             if split == "train":
#                 self.episodes = self.episodes.iloc[:train_len]
#             else:
#                 self.episodes = self.episodes.iloc[train_len:]

#         # -----------------------------------------------------------
#         # 2. ÊûÑÂª∫Á¥¢Âºï
#         # -----------------------------------------------------------
#         self.indices = []
#         for _, row in self.episodes.iterrows():
#             ep_idx = row.get("episode_index")
#             length = row.get("length")
#             chunk_idx = row.get("chunk_index", 0) 
            
#             for frame_idx in range(length):
#                 self.indices.append({
#                     "episode_id": ep_idx,
#                     "frame_idx": frame_idx, 
#                     "chunk_index": chunk_idx
#                 })

#         self.data_cache = {} 
#         print(f"‚úÖ Êï∞ÊçÆÈõÜÂä†ËΩΩÂÆåÊØï: {split} ÈõÜÂåÖÂê´ {len(self.indices)} Â∏ß")

#     def _get_data_chunk(self, chunk_idx):
#         if chunk_idx not in self.data_cache:
#             chunk_name = f"chunk-{chunk_idx:03d}"
#             file_name = f"file-{chunk_idx:03d}.parquet"
#             path = self.root_dir / "data" / chunk_name / file_name
            
#             if not path.exists():
#                 candidates = list((self.root_dir / "data").rglob(f"*{chunk_idx}*.parquet"))
#                 if candidates: path = candidates[0]
#                 else: raise FileNotFoundError(f"Êâæ‰∏çÂà∞Êï∞ÊçÆÊñá‰ª∂: {path}")
            
#             self.data_cache[chunk_idx] = pd.read_parquet(path)
#         return self.data_cache[chunk_idx]

#     def __len__(self):
#         return len(self.indices)

#     def __getitem__(self, idx):
#         item = self.indices[idx]
#         ep_id = item['episode_id']
#         frame_idx = item['frame_idx']
#         chunk_idx = item['chunk_index']
        
#         # --- A. Ëé∑Âèñ Data ---
#         df = self._get_data_chunk(chunk_idx)
#         episode_data = df[df["episode_index"] == ep_id]
        
#         if len(episode_data) == 0: return self._empty_sample()
#         if frame_idx >= len(episode_data): frame_idx = len(episode_data) - 1
            
#         current_data = episode_data.iloc[frame_idx]
        
#         # --- B. Ëé∑Âèñ Video (OpenCV ÁâàÊú¨) ---
#         chunk_str = f"chunk-{chunk_idx:03d}"
#         file_str = f"file-{chunk_idx:03d}.mp4"
#         video_path = self.root_dir / "videos" / self.video_key / chunk_str / file_str
        
#         if not video_path.exists():
#              video_path = self.root_dir / "videos" / chunk_str / file_str
        
#         if video_path.exists():
#             # OpenCV ËØªÂèñÈÄªËæë
#             cap = cv2.VideoCapture(str(video_path))
            
#             # ËÆ°ÁÆóÁªùÂØπÂ∏ßÂè∑ (ÂØπÂ∫î DataFrame ÁöÑ Index)
#             # ÂÅáËÆæ DataFrame ÁöÑ index ‰∏éËßÜÈ¢ëÂ∏ß‰∏•Ê†ºÂØπÂ∫î
#             abs_frame_idx = episode_data.index[frame_idx]
            
#             # ‚ö†Ô∏è Ê≥®ÊÑè: ËøôÈáåÁöÑ index ÂøÖÈ°ªÊòØ chunk ÂÜÖÁöÑÁõ∏ÂØπ‰ΩçÁΩÆ
#             # Â¶ÇÊûú df = read_parquet(chunk_file)ÔºåÂÆÉÁöÑ index ÈªòËÆ§ÊòØ‰ªé 0 ÂºÄÂßãÁöÑ
#             # ÈÇ£‰πàÁõ¥Êé•Áî® abs_frame_idx Â∞±ÊòØÂØπÁöÑ
#             row_in_chunk = df.index.get_loc(abs_frame_idx)
            
#             # Ë∑≥ËΩ¨Âà∞ÊåáÂÆöÂ∏ß
#             cap.set(cv2.CAP_PROP_POS_FRAMES, row_in_chunk)
#             ret, frame_bgr = cap.read()
#             cap.release() # ÂèäÊó∂ÈáäÊîæÔºåÈÅøÂÖçÊñá‰ª∂Âè•ÊüÑËÄóÂ∞Ω
            
#             if ret:
#                 # BGR -> RGB
#                 frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
#                 frame_torch = torch.from_numpy(frame_rgb).permute(2, 0, 1).float() / 255.0
#                 frame_torch = torch.nn.functional.interpolate(frame_torch.unsqueeze(0), size=(self.image_size, self.image_size), mode='bilinear').squeeze(0)
#             else:
#                 print(f"‚ö†Ô∏è ËßÜÈ¢ëËØªÂèñÂ§±Ë¥• (Frame {row_in_chunk}): {video_path}")
#                 frame_torch = torch.zeros(3, self.image_size, self.image_size)
#         else:
#             frame_torch = torch.zeros(3, self.image_size, self.image_size)

#         # --- C. Text & State ---
#         task_text = current_data["task"] if "task" in current_data else "Pick up task"
#         tokens = self.tokenizer(task_text, return_tensors="pt", padding="max_length", truncation=True, max_length=self.max_token_len)

#         state_list = current_data["observation.state"]
#         if hasattr(state_list, 'tolist'): state_list = state_list.tolist()
#         state = torch.tensor(state_list, dtype=torch.float32)
#         if self.normalizer: state = self.normalizer.normalize(state, key="observation.state")

#         # --- D. Action ---
#         actions_chunk_df = episode_data.iloc[frame_idx : frame_idx + self.action_chunk_size]["action"]
#         actions_list = [a.tolist() if hasattr(a, 'tolist') else a for a in actions_chunk_df.values]
#         actions = torch.tensor(np.array(actions_list), dtype=torch.float32)

#         if actions.shape[0] < self.action_chunk_size:
#             pad_len = self.action_chunk_size - actions.shape[0]
#             if actions.shape[0] > 0:
#                 last = actions[-1].unsqueeze(0)
#                 actions = torch.cat([actions, last.repeat(pad_len, 1)], dim=0)
#             else: 
#                 actions = torch.zeros(self.action_chunk_size, 7) 

#         if self.normalizer: actions = self.normalizer.normalize(actions, key="action")

#         return {
#             "observation.images.base_0_rgb": frame_torch,
#             "observation.state": state,
#             "observation.language_instruction.input_ids": tokens["input_ids"].squeeze(0),
#             "observation.language_instruction.attention_mask": tokens["attention_mask"].squeeze(0),
#             "action": actions
#         }

#     def _empty_sample(self):
#         return {
#             "observation.images.base_0_rgb": torch.zeros(3, self.image_size, self.image_size),
#             "observation.state": torch.zeros(7),
#             "observation.language_instruction.input_ids": torch.zeros(self.max_token_len, dtype=torch.long),
#             "observation.language_instruction.attention_mask": torch.zeros(self.max_token_len, dtype=torch.long),
#             "action": torch.zeros(self.action_chunk_size, 7)
#         }

import torch
from torch.utils.data import Dataset
import logging
from pathlib import Path
from transformers import PreTrainedTokenizer
from typing import Optional

# ÂºïÂÖ•ÂÆòÊñπËØªÂèñÂô®
from lerobot.datasets.lerobot_dataset import LeRobotDataset
from utils.normalization import Normalizer

class Pi0Dataset(Dataset):
    def __init__(
        self, 
        root_dir: str, 
        tokenizer: PreTrainedTokenizer,
        normalizer: Optional[Normalizer] = None,
        split: str = "train",
        image_size: int = 224,
        max_token_len: int = 48,
        action_chunk_size: int = 50,
        fps: int = 30,
        video_key: str = "observation.images.cam_high",
    ):
        self.root_dir = Path(root_dir)
        self.tokenizer = tokenizer
        self.normalizer = normalizer
        self.max_token_len = max_token_len
        self.image_size = image_size
        self.action_chunk_size = action_chunk_size
        self.video_key = video_key
        
        logging.info(f"Dataset root: {self.root_dir}")

        # -----------------------------------------------------------
        # 1. ËÆ°ÁÆó Split
        # -----------------------------------------------------------
        temp_dataset = LeRobotDataset(root=self.root_dir, repo_id="dummy_id")
        total_episodes = temp_dataset.num_episodes
        del temp_dataset 
        
        all_indices = list(range(total_episodes))
        train_len = int(total_episodes * 0.95)
        if train_len == 0 and total_episodes > 0: train_len = 1
        
        if split == "train":
            selected_episodes = all_indices[:train_len]
        else:
            selected_episodes = all_indices[train_len:] if train_len < total_episodes else all_indices
            
        print(f"üìä Êï∞ÊçÆÈõÜÂàíÂàÜ ({split}): ÈÄâ‰∏≠ {len(selected_episodes)} / {total_episodes} ‰∏™ Episodes")

        # -----------------------------------------------------------
        # 2. ÂàùÂßãÂåñ LeRobotDataset
        # -----------------------------------------------------------
        dt = 1.0 / fps
        self.delta_timestamps = {
            "action": [i * dt for i in range(action_chunk_size)]
        }

        self.dataset = LeRobotDataset(
            repo_id="local_dataset",
            root=self.root_dir,
            episodes=selected_episodes,
            delta_timestamps=self.delta_timestamps,
            tolerance_s=2 * dt,
            video_backend="pyav" 
        )
        
        # -----------------------------------------------------------
        # 3. ÂÆâÂÖ®Ëé∑Âèñ Stats (ËøôÈáåÂ∞±ÊòØ‰øÆÂ§çÊä•ÈîôÁöÑÂú∞Êñπ üõ°Ô∏è)
        # -----------------------------------------------------------
        self.stats = {}
        try:
            # ÊñπÊ°à A: Â∞ùËØï‰ªé meta ‰∏≠Ëé∑Âèñ (Êñ∞Áâà LeRobot)
            if hasattr(self.dataset, "meta") and hasattr(self.dataset.meta, "stats"):
                self.stats = self.dataset.meta.stats
            # ÊñπÊ°à B: Â∞ùËØïÁõ¥Êé•Ëé∑Âèñ (ÊóßÁâà LeRobot)
            elif hasattr(self.dataset, "stats"):
                self.stats = self.dataset.stats
        except Exception as e:
            print(f"‚ö†Ô∏è Ë≠¶Âëä: Êó†Ê≥ï‰ªé dataset ÂØπË±°‰∏≠Áõ¥Êé•ËØªÂèñ stats ({e})Ôºå‰ΩÜËøô‰∏çÂΩ±ÂìçËÆ≠ÁªÉÔºåÂõ†‰∏∫Êàë‰ª¨Êúâ Normalizer„ÄÇ")

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        # 1. Ëé∑ÂèñÊï∞ÊçÆ
        item = self.dataset[idx]
        
        # 2. ÂõæÁâáÂ§ÑÁêÜ
        raw_image = item[self.video_key]
        if raw_image.shape[1] != self.image_size or raw_image.shape[2] != self.image_size:
            image = torch.nn.functional.interpolate(
                raw_image.unsqueeze(0), 
                size=(self.image_size, self.image_size), 
                mode='bilinear', 
                align_corners=False
            ).squeeze(0)
        else:
            image = raw_image

        # 3. ÊñáÊú¨Â§ÑÁêÜ
        task_text = item.get("task", "Do the task")
        text_tokens = self.tokenizer(
            task_text, 
            return_tensors="pt", 
            padding="max_length", 
            truncation=True, 
            max_length=self.max_token_len
        )

        # 4. ÂΩí‰∏ÄÂåñ
        state = item["observation.state"]
        actions = item["action"]
        
        if self.normalizer:
            state = self.normalizer.normalize(state, key="observation.state")
            actions = self.normalizer.normalize(actions, key="action")

        return {
            "observation.images.base_0_rgb": image,
            "observation.state": state,
            "observation.language_instruction.input_ids": text_tokens["input_ids"].squeeze(0),
            "observation.language_instruction.attention_mask": text_tokens["attention_mask"].squeeze(0),
            "action": actions
        }